[2025-08-02 16:57:27,278: INFO: main: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-02 16:57:27,286: ERROR: main: main: expected str, bytes or os.PathLike object, not NoneType]
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 14, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion.py", line 14, in initiate_data_ingestion
    data_ingestion_config = config.get_data_ingestion_config()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 28, in get_data_ingestion_config
    unzip_dir=Path(config['unzip_dir'])
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\pathlib.py", line 958, in __new__
    self = cls._from_parts(args)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\pathlib.py", line 592, in _from_parts
    drv, root, parts = self._parse_args(args)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\pathlib.py", line 576, in _parse_args
    a = os.fspath(a)
TypeError: expected str, bytes or os.PathLike object, not NoneType
[2025-08-02 17:03:38,580: INFO: main: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-02 17:03:39,273: INFO: data_ingestion: data_ingestion: artifacts\data_ingestion\iris.csv downloaded! Info: 
Connection: close
Content-Length: 3858
Cache-Control: max-age=300
Content-Security-Policy: default-src 'none'; style-src 'unsafe-inline'; sandbox
Content-Type: text/plain; charset=utf-8
ETag: "d1ae283e9a4cd53de5e784edb7fee7b363ca06aef5b946c9fa0574f27d58e2c8"
Strict-Transport-Security: max-age=31536000
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-XSS-Protection: 1; mode=block
X-GitHub-Request-Id: F54B:251B1E:135DDD:330C79:688DF792
Accept-Ranges: bytes
Date: Sat, 02 Aug 2025 11:33:38 GMT
Via: 1.1 varnish
X-Served-By: cache-maa10227-MAA
X-Cache: MISS
X-Cache-Hits: 0
X-Timer: S1754134418.450731,VS0,VE253
Vary: Authorization,Accept-Encoding
Access-Control-Allow-Origin: *
Cross-Origin-Resource-Policy: cross-origin
X-Fastly-Request-ID: d6f56f03da1a23400aadce4b76fc924917576c80
Expires: Sat, 02 Aug 2025 11:38:38 GMT
Source-Age: 0

]
[2025-08-02 17:03:39,275: INFO: data_ingestion: data_ingestion: Skipping extraction as unzip_dir is not set.]
[2025-08-02 17:03:39,276: INFO: main: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-02 21:20:12,894: INFO: main: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-02 21:20:12,905: INFO: data_ingestion: data_ingestion: File already exists at: artifacts\data_ingestion\iris.csv]
[2025-08-02 21:20:12,905: INFO: data_ingestion: data_ingestion: Skipping extraction as unzip_dir is not set.]
[2025-08-02 21:20:12,905: INFO: main: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 17:23:14,825: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 17:23:14,848: ERROR: main: 'dict' object has no attribute 'data_validation']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 15, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 13, in initiate_data_ingestion
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 15, in __init__
    os.makedirs(self.config.data_validation.root_dir, exist_ok=True)
AttributeError: 'dict' object has no attribute 'data_validation'
[2025-08-03 17:24:45,289: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 17:24:45,293: ERROR: main: 'dict' object has no attribute 'data_validation']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 14, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 13, in initiate_data_ingestion
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 15, in __init__
    os.makedirs(self.config.data_validation.root_dir, exist_ok=True)
AttributeError: 'dict' object has no attribute 'data_validation'
[2025-08-03 17:27:53,827: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 17:27:53,830: ERROR: main: DataIngestionConfig.__init__() got an unexpected keyword argument 'source_URL']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 14, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 14, in initiate_data_ingestion
    data_ingestion_config = config.get_data_ingestion_config()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 21, in get_data_ingestion_config
    return DataIngestionConfig(
TypeError: DataIngestionConfig.__init__() got an unexpected keyword argument 'source_URL'
[2025-08-03 17:31:00,268: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 17:31:00,273: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 17:31:00,273: INFO: data_ingestion: Skipping extraction as unzip_dir is not set.]
[2025-08-03 17:31:00,273: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 17:31:00,273: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 17:31:00,274: ERROR: main: 'DataValidationTrainingPipeline' object has no attribute 'run_validation']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 30, in <module>
    data_validation.run_validation()
AttributeError: 'DataValidationTrainingPipeline' object has no attribute 'run_validation'
[2025-08-03 17:47:28,095: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 17:47:28,100: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 17:47:28,101: INFO: data_ingestion: Skipping extraction as unzip_dir is not set.]
[2025-08-03 17:47:28,101: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 17:47:28,101: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 17:47:28,101: ERROR: main: 'DataValidationTrainingPipeline' object has no attribute 'run_validation']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 30, in <module>
    data_validation.run_validation()
AttributeError: 'DataValidationTrainingPipeline' object has no attribute 'run_validation'
[2025-08-03 18:14:08,580: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 18:14:08,590: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 18:14:08,591: INFO: data_ingestion: Skipping extraction as unzip_dir is not set.]
[2025-08-03 18:14:08,592: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 18:14:08,594: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 18:14:08,595: ERROR: main: 'DataValidationTrainingPipeline' object has no attribute 'run_validation']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 30, in <module>
    data_validation.run_validation()
AttributeError: 'DataValidationTrainingPipeline' object has no attribute 'run_validation'
[2025-08-03 18:17:11,962: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 18:17:11,979: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 18:17:11,979: INFO: data_ingestion: Skipping extraction as unzip_dir is not set.]
[2025-08-03 18:17:11,980: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 18:17:11,980: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 18:17:11,981: ERROR: main: 'DataValidationTrainingPipeline' object has no attribute 'run_validation']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 28, in <module>
    data_validation.run_validation()
AttributeError: 'DataValidationTrainingPipeline' object has no attribute 'run_validation'
[2025-08-03 18:21:28,490: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 18:21:28,494: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 18:21:28,504: INFO: data_ingestion: Skipping extraction as unzip_dir is not set.]
[2025-08-03 18:21:28,506: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 18:21:28,507: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 18:21:28,510: ERROR: main: 'DataValidationTrainingPipeline' object has no attribute 'run_validation']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 28, in <module>
    data_validation.run_validation()
AttributeError: 'DataValidationTrainingPipeline' object has no attribute 'run_validation'
[2025-08-03 18:24:05,157: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 18:24:05,166: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 18:24:05,167: INFO: data_ingestion: Skipping extraction as unzip_dir is not set.]
[2025-08-03 18:24:05,168: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 18:24:05,168: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 18:27:30,177: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 18:27:30,186: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 18:27:30,186: INFO: data_ingestion: Skipping extraction as unzip_dir is not set.]
[2025-08-03 18:27:30,187: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 18:27:30,188: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 18:57:20,179: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 18:57:20,188: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 18:57:20,189: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 18:57:20,190: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 18:57:20,197: ERROR: main: DataValidationConfig.__init__() got an unexpected keyword argument 'unzip_data_path']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 27, in <module>
    data_validation.initiate_data_validation()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_validation_pipeline.py", line 13, in initiate_data_validation
    data_validation_config = config.get_data_validation_config()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 32, in get_data_validation_config
    return DataValidationConfig(
TypeError: DataValidationConfig.__init__() got an unexpected keyword argument 'unzip_data_path'
[2025-08-03 19:05:23,999: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 19:05:24,009: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 19:05:24,010: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 19:05:24,012: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 19:09:52,130: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 19:09:52,140: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 19:09:52,141: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 19:09:52,141: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 19:09:52,153: ERROR: main: 'local_data_file']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 27, in <module>
    data_validation.initiate_data_validation()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_validation_pipeline.py", line 13, in initiate_data_validation
    data_validation_config = config.get_data_validation_config()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 33, in get_data_validation_config
    local_data_file=config["local_data_file"],
KeyError: 'local_data_file'
[2025-08-03 19:18:18,770: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 19:18:18,779: ERROR: main: DataIngestionConfig.__init__() got an unexpected keyword argument 'unzip_dir']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 13, in initiate_data_ingestion
    data_ingestion_config = config.get_data_ingestion_config()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 21, in get_data_ingestion_config
    return DataIngestionConfig(
TypeError: DataIngestionConfig.__init__() got an unexpected keyword argument 'unzip_dir'
[2025-08-03 19:19:37,096: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 19:19:37,101: ERROR: main: [Errno 2] No such file or directory: 'config\\schema.yaml']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 12, in initiate_data_ingestion
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 13, in __init__
    self.schema = read_yaml(schema_file_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\utils\common.py", line 8, in read_yaml
    with open(path_to_yaml, 'r') as yaml_file:
FileNotFoundError: [Errno 2] No such file or directory: 'config\\schema.yaml'
[2025-08-03 19:22:50,765: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 19:22:50,772: ERROR: main: [Errno 2] No such file or directory: 'config\\schema.yaml']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 12, in initiate_data_ingestion
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 13, in __init__
    self.schema = read_yaml(schema_file_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\utils\common.py", line 8, in read_yaml
    with open(path_to_yaml, 'r') as yaml_file:
FileNotFoundError: [Errno 2] No such file or directory: 'config\\schema.yaml'
[2025-08-03 19:26:33,609: INFO: utils: Note: NumExpr detected 12 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.]
[2025-08-03 19:26:33,611: INFO: utils: NumExpr defaulting to 8 threads.]
[2025-08-03 19:26:34,568: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 19:26:34,572: ERROR: main: [Errno 2] No such file or directory: 'config\\schema.yaml']
Traceback (most recent call last):
  File "c:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "c:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 12, in initiate_data_ingestion
    config = ConfigurationManager()
             ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 13, in __init__
    self.schema = read_yaml(schema_file_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\utils\common.py", line 8, in read_yaml
    with open(path_to_yaml, 'r') as yaml_file:
         ^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'config\\schema.yaml'
[2025-08-03 19:27:19,540: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 19:27:19,540: ERROR: main: [Errno 2] No such file or directory: 'config\\schema.yaml']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 12, in initiate_data_ingestion
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 13, in __init__
    self.schema = read_yaml(schema_file_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\utils\common.py", line 8, in read_yaml
    with open(path_to_yaml, 'r') as yaml_file:
FileNotFoundError: [Errno 2] No such file or directory: 'config\\schema.yaml'
[2025-08-03 19:28:16,841: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 19:28:16,841: ERROR: main: [Errno 2] No such file or directory: 'config\\schema.yaml']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 12, in initiate_data_ingestion
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 13, in __init__
    self.schema = read_yaml(schema_file_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\utils\common.py", line 8, in read_yaml
    with open(path_to_yaml, 'r') as yaml_file:
FileNotFoundError: [Errno 2] No such file or directory: 'config\\schema.yaml'
[2025-08-03 23:16:44,860: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 23:16:44,866: ERROR: main: [Errno 2] No such file or directory: 'config\\schema.yaml']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 12, in initiate_data_ingestion
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 13, in __init__
    self.schema = read_yaml(schema_file_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\utils\common.py", line 8, in read_yaml
    with open(path_to_yaml, 'r') as yaml_file:
FileNotFoundError: [Errno 2] No such file or directory: 'config\\schema.yaml'
[2025-08-03 23:25:47,315: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 23:25:47,323: ERROR: main: [Errno 2] No such file or directory: 'config\\schema.yaml']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 12, in initiate_data_ingestion
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 13, in __init__
    self.schema = read_yaml(schema_file_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\utils\common.py", line 8, in read_yaml
    with open(path_to_yaml, 'r') as yaml_file:
FileNotFoundError: [Errno 2] No such file or directory: 'config\\schema.yaml'
[2025-08-03 23:26:30,363: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 23:26:30,370: ERROR: main: [Errno 2] No such file or directory: 'config\\schema.yaml']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 12, in initiate_data_ingestion
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 13, in __init__
    self.schema = read_yaml(schema_file_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\utils\common.py", line 8, in read_yaml
    with open(path_to_yaml, 'r') as yaml_file:
FileNotFoundError: [Errno 2] No such file or directory: 'config\\schema.yaml'
[2025-08-03 23:28:18,795: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 23:28:18,803: ERROR: main: [Errno 2] No such file or directory: 'config\\schema.yaml']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 12, in initiate_data_ingestion
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 13, in __init__
    self.schema = read_yaml(schema_file_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\utils\common.py", line 8, in read_yaml
    with open(path_to_yaml, 'r') as yaml_file:
FileNotFoundError: [Errno 2] No such file or directory: 'config\\schema.yaml'
[2025-08-03 23:32:52,419: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 23:32:52,429: ERROR: main: DataIngestionConfig.__init__() got an unexpected keyword argument 'unzip_dir']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 13, in initiate_data_ingestion
    data_ingestion_config = config.get_data_ingestion_config()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 21, in get_data_ingestion_config
    return DataIngestionConfig(
TypeError: DataIngestionConfig.__init__() got an unexpected keyword argument 'unzip_dir'
[2025-08-03 23:34:59,901: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 23:34:59,911: ERROR: main: DataIngestionConfig.__init__() got an unexpected keyword argument 'source_url']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 13, in initiate_data_ingestion
    data_ingestion_config = config.get_data_ingestion_config()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 21, in get_data_ingestion_config
    return DataIngestionConfig(
TypeError: DataIngestionConfig.__init__() got an unexpected keyword argument 'source_url'
[2025-08-03 23:38:20,682: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 23:38:20,687: ERROR: main: DataIngestionConfig.__init__() got an unexpected keyword argument 'source_url']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    data_ingestion.initiate_data_ingestion()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_ingestion_pipeline.py", line 13, in initiate_data_ingestion
    data_ingestion_config = config.get_data_ingestion_config()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 21, in get_data_ingestion_config
    return DataIngestionConfig(
TypeError: DataIngestionConfig.__init__() got an unexpected keyword argument 'source_url'
[2025-08-03 23:47:35,184: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 23:47:35,188: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 23:47:35,188: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 23:47:35,188: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 23:47:35,192: ERROR: data_validation: Exception occurred during data validation: 'DataValidationConfig' object has no attribute 'unzip_data_path']
[2025-08-03 23:47:35,192: ERROR: main: 'DataValidationConfig' object has no attribute 'unzip_data_path']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 27, in <module>
    data_validation.initiate_data_validation()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_validation_pipeline.py", line 16, in initiate_data_validation
    data_validation.validate_all_columns()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_validation.py", line 33, in validate_all_columns
    raise e
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_validation.py", line 16, in validate_all_columns
    data = pd.read_csv(self.config.unzip_data_path)
AttributeError: 'DataValidationConfig' object has no attribute 'unzip_data_path'
[2025-08-03 23:52:33,580: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 23:52:33,584: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 23:52:33,585: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 23:52:33,585: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 23:52:33,590: ERROR: data_validation: Exception occurred during data validation: 'DataValidationConfig' object has no attribute 'unzip_data_path']
[2025-08-03 23:52:33,590: ERROR: main: 'DataValidationConfig' object has no attribute 'unzip_data_path']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 27, in <module>
    data_validation.initiate_data_validation()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_validation_pipeline.py", line 16, in initiate_data_validation
    data_validation.validate_all_columns()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_validation.py", line 33, in validate_all_columns
    raise e
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_validation.py", line 16, in validate_all_columns
    data = pd.read_csv(self.config.unzip_data_path)
AttributeError: 'DataValidationConfig' object has no attribute 'unzip_data_path'
[2025-08-03 23:58:46,036: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-03 23:58:46,041: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-03 23:58:46,041: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-03 23:58:46,041: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-03 23:58:46,057: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-03 23:58:46,058: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-03 23:58:46,058: WARNING: data_validation: Column mismatch found!]
[2025-08-03 23:58:46,059: INFO: main: >>>>>>> Stage Data Validation Stage completed <<<<<<<<

x==========x]
[2025-08-05 16:24:07,254: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-05 16:24:07,304: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 16:24:07,306: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-05 16:24:07,308: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-05 16:24:07,373: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 16:24:07,375: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 16:24:07,376: WARNING: data_validation: Column mismatch found!]
[2025-08-05 16:24:07,377: INFO: main: >>>>>>> Stage Data Validation Stage completed <<<<<<<<

x==========x]
[2025-08-05 16:34:33,554: INFO: main: >>>>>>> Stage Data Ingestion Stage started <<<<<<<<]
[2025-08-05 16:34:33,564: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 16:34:33,565: INFO: main: >>>>>>> Stage Data Ingestion Stage completed <<<<<<<<

x==========x]
[2025-08-05 16:34:33,566: INFO: main: >>>>>>> Stage Data Validation Stage started <<<<<<<<]
[2025-08-05 16:34:33,584: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 16:34:33,584: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 16:34:33,585: WARNING: data_validation: Column mismatch found!]
[2025-08-05 16:34:33,586: INFO: main: >>>>>>> Stage Data Validation Stage completed <<<<<<<<

x==========x]
[2025-08-05 16:41:41,373: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 16:41:41,390: ERROR: data_transformation_pipeline: 'ConfigurationManager' object has no attribute 'get_data_transformation_config']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 10, in <module>
    data_transformation_config = config.get_data_transformation_config()
AttributeError: 'ConfigurationManager' object has no attribute 'get_data_transformation_config'
[2025-08-05 16:49:00,018: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 16:49:00,022: ERROR: data_transformation: Error in data transformation: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
[2025-08-05 16:49:00,022: ERROR: data_transformation_pipeline: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 12, in <module>
    data_transformation.transform_data()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 27, in transform_data
    raise e
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 12, in transform_data
    df = pd.read_csv(self.config.validated_data_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv'
[2025-08-05 16:56:19,388: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 16:56:19,403: ERROR: data_transformation: Error in data transformation: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
[2025-08-05 16:56:19,404: ERROR: data_transformation_pipeline: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 12, in <module>
    data_transformation.transform_data()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 27, in transform_data
    raise e
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 12, in transform_data
    df = pd.read_csv(self.config.validated_data_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv'
[2025-08-05 17:03:56,332: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 17:03:56,341: ERROR: data_transformation_pipeline: "'data_transformation'"]
Traceback (most recent call last):
  File "box\box.py", line 503, in box.box.Box.__getitem__
KeyError: 'data_transformation'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 9, in <module>
    config = ConfigurationManager()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 19, in __init__
    os.makedirs(self.config["data_transformation"]["root_dir"], exist_ok=True)
  File "box\box.py", line 524, in box.box.Box.__getitem__
box.exceptions.BoxKeyError: "'data_transformation'"
[2025-08-05 17:05:42,426: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 17:05:42,439: ERROR: data_transformation: Error in data transformation: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
[2025-08-05 17:05:42,441: ERROR: data_transformation_pipeline: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 12, in <module>
    data_transformation.transform_data()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 27, in transform_data
    raise e
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 12, in transform_data
    df = pd.read_csv(self.config.validated_data_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv'
[2025-08-05 17:06:22,810: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 17:06:22,825: ERROR: data_transformation: Error in data transformation: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
[2025-08-05 17:06:22,826: ERROR: data_transformation_pipeline: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 12, in <module>
    data_transformation.transform_data()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 27, in transform_data
    raise e
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 12, in transform_data
    df = pd.read_csv(self.config.validated_data_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv'
[2025-08-05 17:10:59,168: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 17:10:59,178: ERROR: data_transformation: Error in data transformation: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
[2025-08-05 17:10:59,178: ERROR: data_transformation_pipeline: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 12, in <module>
    data_transformation.transform_data()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 27, in transform_data
    raise e
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 12, in transform_data
    df = pd.read_csv(self.config.validated_data_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv'
[2025-08-05 17:15:36,855: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 17:15:36,874: ERROR: data_transformation: Error in data transformation: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
[2025-08-05 17:15:36,875: ERROR: data_transformation_pipeline: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 12, in <module>
    data_transformation.transform_data()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 27, in transform_data
    raise e
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 12, in transform_data
    df = pd.read_csv(self.config.validated_data_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv'
[2025-08-05 17:27:17,880: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 17:27:17,895: ERROR: data_transformation: Error in data transformation: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
[2025-08-05 17:27:17,895: ERROR: data_transformation_pipeline: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 12, in <module>
    data_transformation.transform_data()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 27, in transform_data
    raise e
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\data_transformation.py", line 12, in transform_data
    df = pd.read_csv(self.config.validated_data_path)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1026, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 620, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1620, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\parsers\readers.py", line 1880, in _make_engine
    self.handles = get_handle(
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\venv\lib\site-packages\pandas\io\common.py", line 873, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_validation/iris.csv'
[2025-08-05 17:56:08,760: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 17:56:08,768: ERROR: data_transformation_pipeline: 'ConfigurationManager' object has no attribute 'get_data_transformation_config']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 10, in <module>
    data_transformation_config = config.get_data_transformation_config()
AttributeError: 'ConfigurationManager' object has no attribute 'get_data_transformation_config'
[2025-08-05 18:04:23,763: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 18:04:23,769: ERROR: data_transformation_pipeline: 'ConfigurationManager' object has no attribute 'get_data_transformation_config']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 10, in <module>
    data_transformation_config = config.get_data_transformation_config()
AttributeError: 'ConfigurationManager' object has no attribute 'get_data_transformation_config'
[2025-08-05 18:08:00,785: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 18:08:00,794: ERROR: data_transformation_pipeline: "'ConfigBox' object has no attribute 'transformed_data_path'"]
Traceback (most recent call last):
  File "box\box.py", line 503, in box.box.Box.__getitem__
KeyError: 'transformed_data_path'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "box\box.py", line 536, in box.box.Box.__getattr__
  File "box\box.py", line 524, in box.box.Box.__getitem__
box.exceptions.BoxKeyError: "'transformed_data_path'"

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "box\box.py", line 538, in box.box.Box.__getattr__
AttributeError: 'ConfigBox' object has no attribute 'transformed_data_path'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "box\config_box.py", line 28, in box.config_box.ConfigBox.__getattr__
  File "box\box.py", line 552, in box.box.Box.__getattr__
box.exceptions.BoxKeyError: "'ConfigBox' object has no attribute 'transformed_data_path'"

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "box\box.py", line 503, in box.box.Box.__getitem__
KeyError: 'transformed_data_path'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "box\box.py", line 536, in box.box.Box.__getattr__
  File "box\box.py", line 524, in box.box.Box.__getitem__
box.exceptions.BoxKeyError: "'transformed_data_path'"

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "box\box.py", line 538, in box.box.Box.__getattr__
AttributeError: 'ConfigBox' object has no attribute 'transformed_data_path'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 10, in <module>
    data_transformation_config = config.get_data_transformation_config()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\config\configuration.py", line 46, in get_data_transformation_config
    transformed_data_path=config.transformed_data_path  # Output path
  File "box\config_box.py", line 30, in box.config_box.ConfigBox.__getattr__
  File "box\box.py", line 552, in box.box.Box.__getattr__
box.exceptions.BoxKeyError: "'ConfigBox' object has no attribute 'transformed_data_path'"
[2025-08-05 18:12:22,374: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 18:12:22,411: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 18:12:22,412: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 18:16:30,793: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 18:16:30,830: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 18:16:30,832: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 18:19:30,790: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 18:19:30,790: ERROR: main: 'DataIngestionTrainingPipeline' object has no attribute 'main']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 13, in <module>
    ingestion.main()
AttributeError: 'DataIngestionTrainingPipeline' object has no attribute 'main'
[2025-08-05 18:26:04,461: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 18:26:04,468: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 18:26:04,469: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 18:26:04,487: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 18:26:04,487: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 18:26:04,488: WARNING: data_validation: Column mismatch found!]
[2025-08-05 18:26:04,489: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 18:26:04,489: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 18:26:04,490: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 18:26:04,495: ERROR: data_transformation_pipeline: Error in Data Transformation Pipeline: 'DataTransformation' object has no attribute 'initiate_data_transformation']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 17, in run
    data_transformation.initiate_data_transformation()
AttributeError: 'DataTransformation' object has no attribute 'initiate_data_transformation'
[2025-08-05 18:26:54,087: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 18:26:54,094: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 18:26:54,094: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 18:26:54,096: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 18:26:54,117: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 18:26:54,117: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 18:26:54,118: WARNING: data_validation: Column mismatch found!]
[2025-08-05 18:26:54,121: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 18:26:54,129: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 18:26:54,129: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 18:26:54,144: ERROR: data_transformation_pipeline: Error in Data Transformation Pipeline: 'DataTransformation' object has no attribute 'initiate_data_transformation']
Traceback (most recent call last):
  File "c:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\data_transformation_pipeline.py", line 17, in run
    data_transformation.initiate_data_transformation()
AttributeError: 'DataTransformation' object has no attribute 'initiate_data_transformation'
[2025-08-05 18:31:13,157: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 18:31:13,165: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 18:31:13,167: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 18:31:13,167: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 18:31:13,180: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 18:31:13,180: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 18:31:13,180: WARNING: data_validation: Column mismatch found!]
[2025-08-05 18:31:13,180: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 18:31:13,180: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 18:31:13,180: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 18:31:13,224: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 18:31:13,224: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 18:31:13,224: INFO: main: >>>>>> Data Transformation Stage completed <<<<<<

x==========x]
[2025-08-05 18:57:45,514: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 18:57:45,522: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 18:57:45,523: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 18:57:45,523: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 18:57:45,543: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 18:57:45,543: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 18:57:45,544: WARNING: data_validation: Column mismatch found!]
[2025-08-05 18:57:45,546: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 18:57:45,546: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 18:57:45,546: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 18:57:45,632: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 18:57:45,632: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 18:57:45,632: INFO: main: >>>>>> Data Transformation Stage completed <<<<<<

x==========x]
[2025-08-05 18:57:45,632: INFO: main: >>>>>> Model Trainer Stage started <<<<<<]
[2025-08-05 18:57:45,689: ERROR: main: [Errno 2] No such file or directory: 'artifacts\\model\\model.pkl']
Traceback (most recent call last):
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\main.py", line 47, in <module>
    trainer.run()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\pipeline\model_trainer_pipeline.py", line 16, in run
    trainer.train_model()
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\components\model_trainer.py", line 25, in train_model
    save_object(self.config.model_path, model)
  File "C:\Users\hr591\OneDrive\Desktop\DatascienceProject\src\datascienceproject\utils\common.py", line 21, in save_object
    with open(file_path, "wb") as file_obj:
FileNotFoundError: [Errno 2] No such file or directory: 'artifacts\\model\\model.pkl'
[2025-08-05 19:15:19,613: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 19:15:19,619: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 19:15:19,622: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 19:15:19,622: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 19:15:19,638: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 19:15:19,639: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 19:15:19,639: WARNING: data_validation: Column mismatch found!]
[2025-08-05 19:15:19,642: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 19:15:19,643: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 19:15:19,644: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 19:15:19,669: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 19:15:19,670: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 19:15:19,671: INFO: main: >>>>>> Data Transformation Stage completed <<<<<<

x==========x]
[2025-08-05 19:15:19,671: INFO: main: >>>>>> Model Trainer Stage started <<<<<<]
[2025-08-05 19:15:19,713: INFO: model_trainer: Training accuracy: 0.9733333333333334]
[2025-08-05 19:15:19,713: INFO: main: >>>>>> Model Trainer Stage completed <<<<<<

x==========x]
[2025-08-05 19:19:51,458: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 19:19:51,467: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 19:19:51,467: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 19:19:51,468: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 19:19:51,482: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 19:19:51,483: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 19:19:51,484: WARNING: data_validation: Column mismatch found!]
[2025-08-05 19:19:51,487: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 19:19:51,487: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 19:19:51,488: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 19:19:51,522: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 19:19:51,523: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 19:19:51,523: INFO: main: >>>>>> Data Transformation Stage completed <<<<<<

x==========x]
[2025-08-05 19:19:51,523: INFO: main: >>>>>> Model Trainer Stage started <<<<<<]
[2025-08-05 19:19:51,570: INFO: model_trainer: Training accuracy: 0.9733333333333334]
[2025-08-05 19:19:51,571: INFO: main: >>>>>> Model Trainer Stage completed <<<<<<

x==========x]
[2025-08-05 19:23:07,358: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 19:23:07,365: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 19:23:07,366: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 19:23:07,366: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 19:23:07,381: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 19:23:07,381: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 19:23:07,381: WARNING: data_validation: Column mismatch found!]
[2025-08-05 19:23:07,383: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 19:23:07,383: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 19:23:07,384: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 19:23:07,410: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 19:23:07,411: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 19:23:07,411: INFO: main: >>>>>> Data Transformation Stage completed <<<<<<

x==========x]
[2025-08-05 19:23:07,412: INFO: main: >>>>>> Model Trainer Stage started <<<<<<]
[2025-08-05 19:23:07,457: INFO: model_trainer: Training accuracy: 0.9733333333333334]
[2025-08-05 19:23:07,458: INFO: main: >>>>>> Model Trainer Stage completed <<<<<<

x==========x]
[2025-08-05 19:26:37,428: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 19:26:37,435: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 19:26:37,439: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 19:26:37,439: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 19:26:37,455: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 19:26:37,456: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 19:26:37,457: WARNING: data_validation: Column mismatch found!]
[2025-08-05 19:26:37,458: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 19:26:37,459: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 19:26:37,459: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 19:26:37,489: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 19:26:37,489: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 19:26:37,489: INFO: main: >>>>>> Data Transformation Stage completed <<<<<<

x==========x]
[2025-08-05 19:26:37,491: INFO: main: >>>>>> Model Trainer Stage started <<<<<<]
[2025-08-05 19:26:37,541: INFO: model_trainer: Training accuracy: 0.9733333333333334]
[2025-08-05 19:26:37,541: INFO: main: >>>>>> Model Trainer Stage completed <<<<<<

x==========x]
[2025-08-05 19:28:06,296: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 19:28:06,301: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 19:28:06,304: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 19:28:06,304: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 19:28:06,314: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 19:28:06,314: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 19:28:06,314: WARNING: data_validation: Column mismatch found!]
[2025-08-05 19:28:06,322: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 19:28:06,322: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 19:28:06,323: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 19:28:06,345: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 19:28:06,346: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 19:28:06,347: INFO: main: >>>>>> Data Transformation Stage completed <<<<<<

x==========x]
[2025-08-05 19:28:06,347: INFO: main: >>>>>> Model Trainer Stage started <<<<<<]
[2025-08-05 19:28:06,389: INFO: model_trainer: Training accuracy: 0.9733333333333334]
[2025-08-05 19:28:06,390: INFO: main: >>>>>> Model Trainer Stage completed <<<<<<

x==========x]
[2025-08-05 19:56:31,204: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 19:56:31,215: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 19:56:31,215: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 19:56:31,216: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 19:56:31,230: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 19:56:31,230: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 19:56:31,231: WARNING: data_validation: Column mismatch found!]
[2025-08-05 19:56:31,233: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 19:56:31,233: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 19:56:31,233: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 19:56:31,264: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 19:56:31,265: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 19:56:31,266: INFO: main: >>>>>> Data Transformation Stage completed <<<<<<

x==========x]
[2025-08-05 19:56:31,267: INFO: main: >>>>>> Model Trainer Stage started <<<<<<]
[2025-08-05 19:56:43,281: INFO: model_trainer: Training accuracy: 0.9733333333333334]
[2025-08-05 19:56:43,287: INFO: main: >>>>>> Model Trainer Stage completed <<<<<<

x==========x]
[2025-08-05 19:56:47,208: INFO: model_trainer: Training accuracy: 0.9733333333333334]
[2025-08-05 20:01:36,994: INFO: main: >>>>>> Data Ingestion Stage started <<<<<<]
[2025-08-05 20:01:37,002: INFO: data_ingestion: File already exists at: artifacts/data_ingestion/iris.csv]
[2025-08-05 20:01:37,002: INFO: main: >>>>>> Data Ingestion Stage completed <<<<<<

x==========x]
[2025-08-05 20:01:37,002: INFO: main: >>>>>> Data Validation Stage started <<<<<<]
[2025-08-05 20:01:37,015: INFO: data_validation: Expected columns: ['sepalLength', 'sepalWidth', 'petalLength', 'petalWidth', 'species']]
[2025-08-05 20:01:37,016: INFO: data_validation: Actual columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']]
[2025-08-05 20:01:37,016: WARNING: data_validation: Column mismatch found!]
[2025-08-05 20:01:37,017: INFO: main: >>>>>> Data Validation Stage completed <<<<<<

x==========x]
[2025-08-05 20:01:37,017: INFO: main: >>>>>> Data Transformation Stage started <<<<<<]
[2025-08-05 20:01:37,017: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation started <<<<<<]
[2025-08-05 20:01:37,042: INFO: data_transformation: Data transformed and saved at artifacts/data_transformation/transformed_iris.csv]
[2025-08-05 20:01:37,042: INFO: data_transformation_pipeline: >>>>>> stage Data Transformation completed <<<<<<

x==========x]
[2025-08-05 20:01:37,042: INFO: main: >>>>>> Data Transformation Stage completed <<<<<<

x==========x]
[2025-08-05 20:01:37,044: INFO: main: >>>>>> Model Trainer Stage started <<<<<<]
[2025-08-05 20:01:38,251: INFO: model_trainer: Training accuracy: 0.9733333333333334]
[2025-08-05 20:01:46,521: INFO: main: >>>>>> Model Trainer Stage completed <<<<<<

x==========x]
[2025-08-05 20:01:46,627: INFO: model_trainer: Training accuracy: 0.9733333333333334]
